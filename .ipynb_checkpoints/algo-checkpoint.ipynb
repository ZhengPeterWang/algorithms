{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4820 Algorithms\n",
    "\n",
    "This is a study note for CS 4820: algorithms. It is meant to be a summary of crucial materials in the course. This was the first time that I wrote notes like this in English.\n",
    "\n",
    "## 0 Introduction: Gale-Shapley Algorithm\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "There are n hospitals and n medical school students. Each hospital has a ranking of students, and each student has a ranking of hospitals. Now we need to match them together. If s prefers h to h', h prefers s to s', but s is matched to h' and h is matched to s', then the matching is said to be unstable. A stable matching is a matching where every pair is not unstable. Find a stable matching between hospitals and students.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Every hospital proposes to the best student in their lists. If the student is not matched, then he/she will agree on matching with the hospital. If, however, the student is matched, then he/she would compare this hospital with the previous one and choose the best one, so that one of these hospitals are rejected. Do until all hospitals are matched with all students.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Since the last hospital proposed to every student, every student and hospital is matched. Also there is no unstable matching. What is more, this algorithm always give hospitals the greatest optimality, while giving students the greatest pessimality. The algorithm has time complexity $O(n^2)$, since at worst case, every hospital proposed to every student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Greedy Algorithms\n",
    "\n",
    "Greedy algorithms involve localized reasoning: You only care about a form of local optimal while progressing through traversal. If the total effect of all local optimal steps is equal to the global optimal, then the problem could be solved by Greedy.\n",
    "\n",
    "### 1.1 Problem: Interval Scheduling\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "There are n requests that occupy time intervals. Each of them starts at s(i) and ends at f(i). Only one request could be processed at a time. Find the maximum amount of requests that could be scheduled without time intervals overlapping with each other.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "While scheduling, always choose the first compatible interval with the earliest f(i)'s.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Proof: By \"Stays Ahead\" Method. We prove that among all possible solutions, our solution has the earliest f(i) for every i. Then we prove by contradiction that our solution is optimal.\n",
    "\n",
    "Run Time: O(n log n), from sorting.\n",
    "\n",
    "### 1.2 Problem: Minimize Lateness Scheduling\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "All intervals are available from point s. Each has length t(i) and deadline d(i). Scheduling intervals after the deadline will have a lateness penality proportional to lateness time. Only one interval can be processed at a time. All intervals should be scheduled. Find a scheduling of these intervals such that the total lateness is minimized.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Always schedule the one with the earliest deadline first.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Proof: By \"Exchange\" Method. First we prove that there is an optimal solution O with no idle time. Then we prove that invert the scheduling of all intervals with deadlines against the \"fully sorted order\" will not increase total lateness. Since intervals can be inverted up until becoming fully sorted by their deadlines, our solution is proved to be at least as good as optimal.\n",
    "\n",
    "### 1.3 Problem: Graph Shortest Paths\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "In a graph where each edge is weighed by l(e) non negative, find a path from s to v with the lowest sum of l(e)'s.\n",
    "\n",
    "#### The Design: Dijkstra's Algorithm\n",
    "\n",
    "Starting from s. Add all nodes already explored to a set S. From all node with at least an edge connecting to S, explore a node v such that $d(v) = min_{e=(u,v):u\\in S}d(u)+l_{e}$ is as small as possible.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Proof: By \"Stay Ahead\" Method. We prove that if another path P' with the shortest distance to v exists, then its cost is no less than the cost of P by the non-negativity of the edge weights and the rule of always finding the least cost edge.\n",
    "\n",
    "Run Time: Using a priority queue, Dijkstra costs O(m) + n O(log n) + m O(log n) for a graph with n nodes and m edges.\n",
    "\n",
    "### 1.4 Problem: Minimum Spanning Tree\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "A **minimum spanning tree** is a subgraph of a connected and edge-weighted graph that has all nodes in the original graph and has the least total cost possible. Find the minimum spanning tree of a connected edge-weighed graph.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "**Kruskal's Algorithm**: Start from no edges at all, and insert edges by increasing cost. We only insert those which do not create cycles.\n",
    "\n",
    "**Prim's Algorithm**: Start from any node s, build up set S by adding node v that minimizes the attachment cost $min_{e=(u,v): u \\in S} C_e$ to S. The edges added form a minimum spanning tree.\n",
    "\n",
    "**Reverse-Delete Algorithm**: Start from the full graph, delete edges by decreasing cost. Only delete those that will not make the graph disconnected.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Proof: First a lemma called \"Cut Property\" saying that the minimum spanning tree of G contains the lowest cost edge e connecting a subgraph S and other parts of G. This is proven by \"Exchange Method\", showing that  edges of other spanning trees could be exchanged with e and lower its cost. The counterpart of that lemma in \"Reverse Delete\" is \"Cycle Property\", that the minimum spanning tree does not contain the highest cost edge e in any cycle C in G. Then we prove that output of Kruskal's and Prim's are spanning trees.\n",
    "\n",
    "Run Time: Prim's Algorithm is nearly the same as Dijkstra's, which is O(m + n log n + m log n) while using a priority queue. \n",
    "\n",
    "#### Appendix: The Union Find Data Structure\n",
    "\n",
    "Kruskal's Algorithm needs a data structure that could find whether a node is in a connected component and union two connected components. They require the Union Find data structure. \n",
    "\n",
    "This efficient implementation of Union Find below gives a pointer to each node, initially pointed to itself. Unioning two components only require us to change one pointer, and we always change the pointer of the component with the lower size to point to the component with the higher size. Find traces the pointer chain, and while done, pointing every node on the path to the root. Union takes O(1), Find takes O(log n). \n",
    "\n",
    "Kruskal's Algorithm, implemented by Union Find, takes O(m log n) time.\n",
    "\n",
    "#### Problem Related: Huffman Code\n",
    "\n",
    "Prefix codes are a function f that maps $x \\in S$ to a sequence of 0's and 1's such that if $x \\neq y$, then f(x) is not a prefix of f(y). Notice that a binary tree can express prefix codes.\n",
    "\n",
    "Since an optimal prefix code would let the lowest frequency word to have the longest path, we could build the tree bottom up. Specifically, union the two lowest frequency words into one word (node), and build up the tree by adding a parent to them. Continue until all words are fitted into the tree. The total running time is O(k log k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Divide and Conquer\n",
    "\n",
    "Divide and Conquer is a way of dividing a problem into several non-overlapping subproblems, recursively cracking them individually, and unioning them together to get the solution of the big problem. The Master Theorem is the reason why this tactic would reduce time complexity.\n",
    "\n",
    "### 2.1 The Master Theorem\n",
    "\n",
    "In Divide and Conquer, a recurrence relation\n",
    "\n",
    "\\begin{equation*}T(n) = aT(n/b) + f(n)\\end{equation*}\n",
    "\n",
    "is very common. The complexity of T(n) is given by the Master Theorem:\n",
    "\n",
    "1. Suppose $\\exists \\epsilon > 0$ such that $ f(n) = O(n^{log_{b}a} - \\epsilon )$, then $T(n) = \\Theta(n^{log_{b}a})$.\n",
    "\n",
    "2. Suppose $\\exists k \\geq 0$ such that $f(n) = \\Theta(n^{log_{b}a}log^{k}n) $, then $T(n) = \\Theta(n^{log_{b}a}log^{k+1}n)$.\n",
    "\n",
    "3. Suppose $\\exists \\epsilon > 0$ such that $f(n) = \\Omega(n^{log_{b}a} + \\epsilon )$, but $\\exists c > 0$ and $n$ such that $af(n/b) \\leq cf(n)$, then $T(n) = \\Theta(f(n))$.\n",
    "\n",
    "### 2.2 Problem: Inversion Number\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "In a sequence a(i), if i < j but a(i) > a(j), then this is called an inversion. Find the number of inversions in a.\n",
    "\n",
    "#### The Design and the Analysis\n",
    "\n",
    "The process of counting the number of inversions can be done in the same way as merge sorting the list. While merging, maintain two counters and track the current smallest value not sorted. If the current smallest value is in the second list, then inversion counter should be added by the number of remaining elements in the first list. The run time is O(n log n).\n",
    "\n",
    "### 2.3 Problem: Closest Pair of Points\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Find the closest pair of points among n points in a plane.\n",
    "\n",
    "#### The Design and the Analysis\n",
    "\n",
    "1-D version is easier to be solved. We first sort these points at O(n log n) and then traverse the line at O(n) to find the closest pair.\n",
    "\n",
    "2-D version can be cracked by d/c. Divide the plane into the left plane and the right plane and do the algorithm separately. We now know the closest distance of point pairs within these two parts, and the minimum of them is called d'. Note that while combining the left part and the right part, the only points that we should consider are those near the border. For each point near the border, we only need to consider a constant amount of points near it, so the \"merge\" complexity is O(n). By the Master Theorem, the total complexity is O(n log n).\n",
    "\n",
    "### 2.4 Problem: Convolution & FFT\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "The **convolution** of two vectors a and b is $$a * b = (a_0 b_0, a_0 b_1 + a_1 b_0, a_0 b_2 + a_1 b_1 + a_2 b_0, ..., a_{n-2}b_{n-1} + a_{n-1}b_{n-2}, a_{n-1}b_{n-1})$$ a vector of diagonal sums. Find the convolution of two vectors efficiently.\n",
    "\n",
    "#### The Design and the Analysis: Fast Fourier Transform\n",
    "\n",
    "Think about multiplying two polynomials, and the coefficients of the resulted polynomial is the convolution of the coefficients of the previous polynomials. A way to multiply P and Q is to \n",
    "\n",
    "(1) evaluate 2n values of P and 2n values of Q.\n",
    "\n",
    "(2) multiply these values one by one to get 2n values of PQ.\n",
    "\n",
    "(3) Since PQ is at most of degree 2n - 2, these calculated values determine the formula of PQ, by the fundamental theorem of arithmatic.\n",
    "\n",
    "(2) can be done in O(n). (1) is harder to do and needs d/c. Instead of evaluating P and Q at random values, we evaluate P and Q at 2nth roots of unity. We divide P into two polynomials: Pl with even terms and Pr with odd terms. They should be evaluated at nth roots of unity. Since $P(x) = Pl(x^2)+xPr(x^2)$, $P(\\omega_{j, 2n})=Pl(\\omega_{j,2n}^2)+\\omega_{j,2n}Pr(\\omega_{j,2n}^2)$. Since $\\omega_{j, 2n}^2 = \\omega_{j/2, n}$, this means we now have a recurrence relation $T(n) \\leq 2T(n/2) + O(n)$, which gives us O(n log n) complexity.\n",
    "\n",
    "(3) requires polynomial interpolation, which is done by Discrete Fourier Transform, summed up in below:\n",
    "\n",
    "For any polynomial $C(x) = \\sum_{s = 0}^{2n-1}c_s x^s$, and corresponding polynomial $D(x) = \\sum_{s = 0}^{2n-1}C(\\omega_{s, 2n}) x^s$, we have that $c_s = \\frac{1}{2n}D(\\omega_{2n - s, 2n})$.\n",
    "\n",
    "Evaluating values $D(\\omega_{2n-s,2n})$ is the same as in (1), which takes O(n log n) time. This whole practice of polynomial multiplication (and convolution calculation) thus takes O(n log n) time.\n",
    "\n",
    "### 2.5 Problem: Matrix Multiplication Optimization\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "The naive way of multiplying two matrixes of n * n will generate a time complexity of $O(n^3)$. Can we do better, even just a little bit? Also, the naive way of multiplying two numbers takes $O(n^2)$.\n",
    "\n",
    "#### The Design and the Analysis: Strassen's Algorithm\n",
    "\n",
    "The idea came from exploiting d/c. Every matrix can be divided into four blocks, and one matrix multiplication can thus become eight smaller matrix multiplications, and the recurrence relation would be $T(n) = 8T(n/2) + O(n)$, giving a complexity of $O(n^3)$. But not all of the eight are necessary: in fact, by reorganizing terms and exploiting the easy-to-calculate plusses, we only need seven matrix multiplications, which would give recurrence relation $T(n) = 7T(n/2) + O(n)$ and complexity $O(n^{log_{2}7}) \\approx O(n^{2.8074})$.\n",
    "\n",
    "#### Problem Related: Integer Multiplication Optimization\n",
    "\n",
    "Optimizing integer multiplication is very similar to Strassen's Algorithm. Since $$xy = (x_1 2^{n/2} + x_0)(y_1 2^{n/2} + y_0) = x_1 y_1 2^n + (x_1 y_0 + x_0 y_1)2^{n/2} + x_0 y_0,$$ and the middle term $$ x_1 y_0 + x_0 y_1 = (x_1 + x_0) (y_1 + y_0) - x_1 y_1 - x_0 y_0,$$ the four required multiplications could be compressed into three. Therefore the complexity of this algorithm becomes $O(n^{log_{2} 3}) \\approx O(n^{1.58})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dynamic Programming\n",
    "\n",
    "Dynamic programming is an extension of Greedy and Divide and Conquer. You divide a problem into a polynomial number of possibly-overlapping subproblems, crack them individually and recursively, and merge them together through a recurrence relation. Note that the overlapping feature of subproblems typically require memoization and solving subproblems first via iteration.\n",
    "\n",
    "### 3.1 Problem: Weighted Interval Scheduling\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "This is an extension to Problem 1.1. There are n requests, each with starting time s(i) and finishing time f(i), also a weight v(i). Find a scheduling scheme such that no intervals overlapping are selected, and the total weight is maximized.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Suppose opt(j) is the maximum total weight that could be generated by intervals 1, ..., j. Then\n",
    "\n",
    "\\begin{equation*}opt(j) = max(v_j + opt((p(j)), opt(j - 1)),\\end{equation*}\n",
    "\n",
    "where p(j) is the right most interval that does not overlap with j. Also, opt(0) = 0.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "The memoized version has O(n) complexity after we sort the intervals. Even recording the trace of the scheduling process does not need more than O(n). Changing the structure of the algorithm from top-down recursion to bottom-up iteration would reduce the extra cost of function calls and would be more efficient.\n",
    "\n",
    "### 3.2 Problem: Segmented Least Squares\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Fit the data by the model of segmented lines. Specifically, adding an extra line into the model would involve penalty, and errors between data and model would also involve penalty. The goal is to find a solution with the least penalty.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Suppose opt(j) is the optimal solution with the first j data points (the solution to the problem would be opt(n)), then\n",
    "\n",
    "\\begin{equation*}opt(j) = \\min_{1 \\leq i \\leq j}(e_{i,j} + C + opt(i-1)).\\end{equation*} \n",
    "\n",
    "Here, C is the penalty of adding a new line, and $e_{i,j}$ is the sum of errors of the data points between i and j if you draw the least-square best fit line between these two. The base case would be opt(0) = 0.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Calculating one least square error $e_{i,j}$ can be cleverly done in O(1), and calculating all of them would require $O(n^2)$. Calculating opt's then requires $O(n^2)$.\n",
    "\n",
    "### 3.3 Problem: Knapsack\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "There are n objects, each with a value v(i) and a weight w(i). You have a knapsack with capacity W. Find a way to fill maximum value into your knapsack.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "If you have the first i objects and capacity w, the optimal value in the knapsack would be opt(i, w). Thus\n",
    "\n",
    "\\begin{equation*} opt(i, w) = max(opt(i-1, w), v_i + opt(i-1, w-w_i)). \\end{equation*}  \n",
    "\n",
    "Also, opt(0, x) = 0 for all x and opt(y, 0) = 0 for all y.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "This program runs in O(nW) time with bottom-up iteration. This is a problem known as pseudo-polynomial time, because the complexity is proportional to W, but input W only takes log W digits, so the time complexity is exponential with regard to input size.\n",
    "\n",
    "### 3.4 Problem: RNA Secondary Structure\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "RNAs can fold with itself. It is a chain of bases A, U, C, G. A pairs with U, and C pairs with G. RNAs with most base connecting pairs have the lowest energy. But not all pairs are possible. Specifically, (1) the connecting point between two bases of a pair must be at least 4 bases apart (no sharp turn). (2) No base appears in more than one pair. (3) No pair connections should cross with each other. Find the pairing of a RNA string with the lowest energy.\n",
    "\n",
    "#### The Design and the Analysis\n",
    "\n",
    "This is a dynamic programming over intervals. If you consider the maximum number of pairs between base i and j as opt(i, j), then it satisfies\n",
    "\n",
    "(1) if $i \\geq j - 4$, then opt(i, j) = 0.\n",
    "\n",
    "(2) all other cases, $$opt(i, j) = max(opt(i, j - 1), max(1 + opt(i, t - 1) + opt(t + 1, j - 1))),$$ where the second max is over all t's between i and j such that $b_t$ and $b_j$ forms a valid base pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNAstructure(self):\n",
    "    # for all i, j\n",
    "    if i >= j - 4:\n",
    "        opt[i][j] = 0;\n",
    "    for k in range(5, n):\n",
    "        for i in range(1, n - k + 1):\n",
    "            j = i + k;\n",
    "            maxn = opt[i][j - 1];\n",
    "            for t in range(i, j):\n",
    "                if isValidPair(t, j):\n",
    "                    maxn = max(maxn, 1 + opt[i][t - 1] + opt[t + 1][j - 1]);\n",
    "            opt[i][j] = maxn;\n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are three iterations in the algorithm above, the time complexity is $O(n^3)$.\n",
    "\n",
    "### 3.5 Problem: Sequence Alignment\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Two strings need to be aligned with each other. You can add characters to strings or change characters in the strings, each with penalty. Find a way to align strings together with the lowest penalty.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Let opt(i, j) be the minimum cost of aligning the first i characters of string A and first j characters of string B together. Then\n",
    "\n",
    "\\begin{equation*}opt(i, j) = min(a_{x_i y_j} + opt(i - 1, j - 1), \\delta + opt(i - 1, j), \\delta + opt(i, j - 1)).\\end{equation*}\n",
    "\n",
    "Where $a_{x_i y_j}$ is the replacement penalty of the ith charater of the first word and jth character of the second, and $\\delta$ being the gap penalty. Also, opt(0, n) = opt(n, 0) = n.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Correctness could be proved by induction on i+j. Time complexity is O(mn), where m, n are lengths of the strings.\n",
    "\n",
    "#### Problem Related: Longest Common Subsequence\n",
    "\n",
    "Find the longest common subsequence of two strings is also a DP problem. If $a_i = b_j$, then opt(i, j) = 1 + opt(i - 1, j - 1), otherwise opt(i, j) = max(opt(i, j - 1), opt(i - 1, j)), and opt(i, 0) = opt(0, j) = 0.\n",
    "\n",
    "### 3.6 Problem: Graph Shortest Path\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Different from Problem 1.3, this time we allow negative edges in a graph, but still no negative cycles. Find the  path with the lowest weight from u to v in this graph.\n",
    "\n",
    "#### The Design and the Analysis: Bellman-Ford Algorithm\n",
    "\n",
    "Let opt(i, v) be the minimum cost of a v-t path using at most i edges. Then\n",
    "\n",
    "\\begin{equation*} opt(i, v) = min(opt(i - 1, v), \\min_{w \\in V}(opt(i-1, w) + c_{vw})).\\end{equation*}\n",
    "\n",
    "Also, opt(0, t) is 0, and for all v in V but not t, opt(0, v) is initially infinity. We just need to calculate opt(n, s). Note that i has a meaning other than the number of edges of the current optimal path: it could just be a counter -- at the ith iteration, all updates to opt's will correspond to the discovery of a shorter path with i edges to t. \n",
    "\n",
    "An important fact guarantees the correctness of this algorithm: if the graph does not have a negative cycle, then its shortest path must have number of edges less than n. If after n-1 iterations, there exists some v and w such that $opt(n-1, v) > opt(n - 1, w) + c_{vw}$, then there must be a negative cycle.\n",
    "\n",
    "The recurrence relation could be further simplified into this:\n",
    "\n",
    "\\begin{equation*} M(v) = min(M(v), \\min_{w \\in V}(M(w) + c_{vw})) \\end{equation*}\n",
    "\n",
    "running (n-1) times on all nodes $v \\in V$. Note that depending on the execution sequence of nodes, it is possible that in the ith iteration, a path having more than i edges could be found, but we could guarantee that all paths having i edges towards the destination would be found after the ith iteration of the algorithm.\n",
    "\n",
    "The time complexity of this algorithm is O(mn). During each iteration, each node should visit all edges that it could reach in one step to check for updates, so in one iteration, all edges are traversed, and there are n iterations, so time complexity is O(mn). \n",
    "\n",
    "#### Further Reading: SPFA\n",
    "\n",
    "A lot of checks of adjacent nodes of v is unnecessary; if we could put those newly-updated nodes into a queue, then every time we only need to update those v's affected by nodes in the queue. This is the idea of Shortest Path Faster Algorithm, and its complexity is O(m).\n",
    "\n",
    "#### Problem Related: Floyd-Warshall Algorithm\n",
    "\n",
    "There is a related algorithm that could calculate the shortest path between all nodes in a graph at $O(n^3)$. \n",
    "\n",
    "If we define opt(i, j, k) as a function that could return the shortest path from i to j using nodes with label of at most k, then\n",
    "\n",
    "(1) $opt(i, j, 0) = w(i, j)$, and\n",
    "\n",
    "(2) $opt(i, j, k) = min(opt(i, j, k - 1), opt(i, k, k - 1) + opt(k, j, k - 1))$.\n",
    "\n",
    "As a result, we could write the following algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FloydWarshall(self):\n",
    "    # already set opt[i][j] = w(i, j) if they are connected by edges\n",
    "    # also set opt[i][i] = 0\n",
    "    for k in range(1, n + 1):\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if opt[i][j] > opt[i][k] + opt[k][j]:\n",
    "                    opt[i][j] = opt[i][k] + opt[k][j];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the case of Bellman-Ford, we simplify the recurrence relation by eliminating k in opt function, and now k loses its previous character: it simply becomes a counter. Now at kth iteration, opt(i, j) must have considered the shortest path from i to j using nodes with label of at most k, though it could also consider those paths passing through nodes with labels greater than k. This proves that after k iterations, opt function would return a correct solution.\n",
    "\n",
    "Floyd-Warshall could also detect negative cycles. If, after k iterations, for some i opt(i, i) < 0, then there will be a negative cycle.\n",
    "\n",
    "Note that Kleene's Theorem that \"every language accepted by a finite automaton is regular\" is proved in a dynamic programming way very similar to Floyd Warshall Algorithm.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Here is a procedure of using dynamic programming:\n",
    "\n",
    "(1) Find an opt function. Try to find variables that could describe the state of the original problem, start from cases where these variables are small, and consider whether they will help you to divide the problem into polynomial numbers of similar subproblems.\n",
    "\n",
    "(2) Find the recurrence relation between values of the opt function. Remember that opt of bigger values may be a function of a huge number of opt of smaller values.\n",
    "\n",
    "(3) Transform the problem from top-down recursion into bottom-up iteration. The standard procedure is (1) draw a huge table. (2) draw all the values that opt(i, j, ...) depends on. (3) Find the starting point, ending point, and the steps of the iterations. Remember that later-calculated opt values can only depend on previous-calculated opt values. If not, then the problem is impossible to be done by dynamic programming.\n",
    "\n",
    "(4) Optimize space complexity of the problem by removing variables unnecessary in opt or reuse array spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Network Flow\n",
    "\n",
    "Network Flow is another efficient algorithm that considers edge selections and node selections of a graph. The most important characteristic of network flow algorithms is that they could undo themselves while traversing all possible solutions, yet progress are still guaranteed to be made in polynomial time. Maximum Flow and Minimum Cut are very common models in graph problems, as well as problems like circulation.\n",
    "\n",
    "### 4.1 The Problem and Ford-Fulkerson Algorithm\n",
    "\n",
    "#### The Problem: Network Flow\n",
    "\n",
    "Given a directed integer-weighted graph with a single source and a single sink. A **s-t flow** is a function $f: E  \\rightarrow R+$ that satisfies the following two properties:\n",
    "\n",
    "(1) $ \\forall e \\in E, 0 \\leq f(e) \\leq c_e. $\n",
    "\n",
    "(2) $ \\forall v \\neq s, t, \\sum_{e into v}f(e) = \\sum_{e out of v} f(e).$\n",
    "\n",
    "The **value** of a flow $v(f) = \\sum_{e out of s}f(e) - \\sum_{e into s}f(e)$.\n",
    "\n",
    "Find the maximum flow value in this graph.\n",
    "\n",
    "#### The Design: Ford-Fulkerson Algorithm\n",
    "\n",
    "The way to solve this problem is to alter the graph constantly. When a flow of certain amount goes through an edge, its capacity is reduced by the flow amount, while its reverse \"undo\" capacity increases by the flow amount. The \"undo\" backward edges newly created have the same status as the original forward edges, and this new graph is called a residual graph. The algorithm suggests that we should augment possible paths by searching for possible channels from source to sink, and create the corresponding residual graphs, treating back edges same as forward edges, augment again, until it was impossible to augment any more. The flow at that time would be maximum flow.\n",
    "\n",
    "#### The Analysis: Maximum Flow is Minimum Cut\n",
    "\n",
    "We first note that Ford-Fulkerson would terminate, and the complexity of Ford-Fulkerson is O(Cm), where C is the  sum of capacity of all outbound edges from s. \n",
    "\n",
    "Define a **s-t cut** as a partition (A,B) of V so that s is in A and t is in B. We now can prove these facts:\n",
    "\n",
    "(1) Let f be any s-t flow, and (A,B) be any s-t cut. Then v(f) = fout(A) - fin(A).\n",
    "\n",
    "(2) $v(f) \\leq c(A,B)$, where $c(A,B) = \\sum_{e out of A}c_e$. Or, the value of every flow is bound by every cut. Notice that capacity of the cut depends only on capacities of outward edges.\n",
    "\n",
    "(3) If v(f) = c(A,B), then f is the maximum flow and (A,B) is the minimum cut. \n",
    "\n",
    "(4) (Max-Flow-Min-Cut Theorem) Value of a maximum flow is the capacity of a minimum cut, and in this case, no augmented paths could be found.\n",
    "\n",
    "(5) If all capacities in the flow network are integers, then there is a maximum flow f for which every flow value f(e) is an integer.\n",
    "\n",
    "#### Optimization: Choosing good augment paths\n",
    "\n",
    "Ford-Fulkerson did not specify how we should choose good augment paths. There is a good choice of augment paths that can reduce the complexity of the algorithm to $O(m^2log_2C)$. At the first iteration, we only consider those paths in the residual graph with capacity greater than $\\delta = C/2$. We can prove that each round would augment at most O(m) times, with complexity $O(m^2)$. Then drop $\\delta$ by factors of 2. There would be at most $O(log_2C)$ rounds of iterations, thus the total complexity is $O(m^2log_2C)$. This proves that Ford-Fulkerson is polynomial, and not pseudo-polynomial.\n",
    "\n",
    "Further optimization of augment path choice is possible. We could also choose the shortest (least edges) augment path currently available via breadth-first search. The length of the shortest path never decreases, and after every m augmentations, the length of the shortest path must increase. This shows that Ford-Fulkerson could have complexity $O(m^2n)$. This algorithm is called Edmonds-Karp Algorithm.\n",
    "\n",
    "Among all shortest augment paths, we could traverse them in an ordered way so that previous calculation effort of augment paths could be utilized, which is called Dinitz's Algorithm. Divide the shortest path scheme into (n-1) phases: in each phase the length of the shortest augment path does not change. Start from s and draw the level graph of G ($L_G$). If we could reach t in $L_G$, then augment the flow, update the level graph, and restart. If we couldn't, we would delete that point, retreat, and find some other paths (we could not find any augment path that goes through the node that we deleted just now, so we don't need to traverse any edges that go through this node in later BFS, and it is this that saves time). There are at most m augmentations in each phase, each covers at most n steps, and there are at most (n-1) phases, which means Dinitz's has complexity $O(mn^2)$.\n",
    "\n",
    "### 4.2 Problem: Bipartite Matching\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "A matching of a bipartite graph is a series of disjoint edges in the graph. We want to find the maximum matching. \n",
    "\n",
    "#### The Design\n",
    "\n",
    "Adding a source and a sink to the bipartite graph would transform it into a graph similar to the network flow problem described above, and if we give each edge capacity 1, then the bipartite graph would be a unit-capacity flow graph. Each vertex has only one inbound capacity, and could have no more than one outbound flow. Thus no edges in the original bipartite graph selected by maximum flow could be adjoint. The maximum flow in the graph is thus the maximum number of edges in the matching. \n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "Plugging in complexities above, we find the complexity of bipartite matching to be O(mn). \n",
    "\n",
    "#### Further Reading: Hall's Theorem\n",
    "\n",
    "An analysis of the minimum cut in Bipartite Matching gives us a new insight: if a bipartite graph has a maximum flow less than n, then there must exist a subset A in X such that its neighbors $|\\Gamma(A)| < |A|$. Conversely, a bipartite graph has a perfect matching iff for all subsets A in X, $|\\Gamma(A)| \\geq |A|$. This is Hall's Theorem, a special case of Max Flow Min Cut Theorem. A corollary of this theorem is that all regular bipartite graphs have perfect matching. The theorem gives us a way to find whether a perfect matching in a bipartite graph exists in O(mn) time.\n",
    "\n",
    "### 4.3 Problem: Circulation with Supplies and Demands\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given a directed integer-weighted graph. A **circulation** is a function $f: E  \\rightarrow R+$ that satisfies the following two properties:\n",
    "\n",
    "(1) $\\forall e \\in E, l(e) \\leq f(e) \\leq c(e)$.\n",
    "\n",
    "(2) $\\forall v \\in V, \\sum_{e into v}f(e) - \\sum_{e out of v}f(e) = d(v)$.\n",
    "\n",
    "Here l(e) is the lower limit of the flow in an edge, c(e) the upper limit, and d(v) is the demand of vertex v. If d(v) < 0, then v is supplying the flow. Find out whether a circulation exists, that is, whether every node's demand and supply could be satisfied.\n",
    "\n",
    "#### The Design and the Analysis\n",
    "\n",
    "Multi-source multi-sink network flows could be transformed into single-source single-sink network flows. The circulation can be treated as a superposition of a basic flow that has value l(e) on every edge e, and an extra flow that we would try our best not to exceed c(e) - l(e) at every e, while satisfying the revised demand d'(v). \n",
    "\n",
    "Then like in the case of bipartite graphs, we create a hypothetical source s' and sink t'. Every node v with d'(v) > 0 would connect to t' with an edge of capacity d'(v), and those with d'(v) < 0 would connect to s' with an edge of capacity -d'(v). Right now the problem of circulation existence becomes a problem of whether the transformed graph has a maximum network flow equal to $\\sum_{e into t'}c(e)$. If $\\sum_{e into t'}c(e) > \\sum_{e out of s'}c(e)$, then total demand is greater than total supply, and if maximum flow now equals to $\\sum_{e out of s'}c(e)$, then we could know that capacity of the network would not become a constraint of resource distributions.\n",
    "\n",
    "### 4.4 Problem: Survey Design\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "There are n consumers and k products, and each consumer has purchased several products. Now you want to design a way to distribute surveys such that consumer i would receive no less than $c_i$ surveys and no more than $c_i'$ surveys, and product j would receive no less than $p_j$ reviews and no more than $p_j'$ reviews. \n",
    "\n",
    "#### The Design\n",
    "\n",
    "This problem could be reduced to a circulation problem. Hypothetically create a source s* and sink t*. The source has edges pointing to each consumer i, with $l(e_i) = c_i, c(e_i) = c_i'$. The sink has edges received from each product j, similar to the logic of consumers. The middle part of the graph is bipartite, with max capacity 1 and min capacity 0. Now it is a complete circulation.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "From a way of designing a survey, we could construct a flow of the said network. Conversely, since the circulation problem is integer-valued, once a circulation exists, then a survey design would also be available.\n",
    "\n",
    "### 4.5 Problem: Airline Scheduling\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "This problem is greatly simplified. You have n flights and k planes. If flight j could be scheduled after flight i with a single plane, then j is said to be reachable from i. Given the reachability conditions of said flights, find whether scheduling them with at most k planes is possible.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "We now reduce this problem to circulation. Flight i is modelled by an edge $(u_i, v_i)$, lower bound 1, capacity 1. Reachable flights are $(v_i, u_j)$, lower bound 0, capacity 1. For each i, there is $(s, u_i)$, with lower bound 0, capacity 1. For each j, there is $(v_j, t)$, lower bound 0, capacity 1. Also, there is an edge (s, t), lower bound 0, capacity k. s has demand -k, t has demand k, all other nodes have demand 0. Now it is a total circulation.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "If there is a way to perform flights using $k' \\leq k$ planes, then the set of flights correspond to the flow paths in the network. Those k-k' planes could be transported by (s,t) edge. Thus a solution of airline scheduling would imply a solution of such circulation. Conversely, a valid circulation in our network constructed (acyclic) implies k' paths of valid plane routes: At all nodes between s and t, paths just need to follow the flow. Since this graph is unit-capacity, all vertexes have at most one flow in, one flow out. And planes just need to follow the paths.\n",
    "\n",
    "### 4.6 Problem: Image Segmentation\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "If you have played with PhotoShop, you would know that there is a handy tool (magic stick) that could select foreground objects out of the background. Say each pixel of the image has a likelihood $a_i$ to belong to foreground, and likelihood $b_i$ to belong to background. Separating a pixel i from its neighboring pixel j would involve a penalty $p_{ij}$. Find a partition (A, B) such that $$q(A,B) = \\sum_{i \\in A} a_i + \\sum_{j \\in B}b_j - \\sum_{(i, j) \\in E, | A \\cap \\{ i, j\\}| = 1}p_{ij}$$ is maximized.\n",
    "\n",
    "#### The Design and the Analysis\n",
    "\n",
    "First, maximizing q(A, B) is also minimizing $$q'(A, B) = \\sum_{i \\in A} b_i + \\sum_{j \\in B}a_j + \\sum_{(i, j) \\in E, | A \\cap \\{ i, j\\}| = 1}p_{ij}.$$ We now create a \"supersource\" s to represent foreground, a \"supersink\" t to represent background. Each pixel can be connected from s, and be connected to t. Undirected edges become two directed edges. \n",
    "\n",
    "How do we weigh these edges? Clearly, (s, j) edges have weight $a_j$ (when j in B, this constitutes the cut), (i, t) edges have weight $b_i$ (when i in A, this constitutes the cut), and (i, j) edges have weight $p_{ij}$ (when i in A, j in B, this constitutes the cut). Very importantly, c(A, B) only contains weights of outward edges that cross the boundary between A and B, so on the (A, B) boundary, two $p_{ij}$'s are only counted once. Now that c(A, B) = q'(A, B), our job is to find the minimum cut of this graph.\n",
    "\n",
    "### 4.7 Problem: Project Selection\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given a set of projects, each with benefit $p_i$ (could be negative). Some of them are prerequisites of others, and a project could not be done until all its prerequisites are done. Find a project selection that could maximize total benefit.\n",
    "\n",
    "#### The Design\n",
    "\n",
    "Dynamic programming will not work because it is hard to find out one or two indicator variables in the graph that could show progress. Our goal is to reduce this problem to minimum cut. \n",
    "\n",
    "The prerequisites already form a directed graph. Let (i,j) be an edge iff j is the prerequisite of i. Now we add a source s and a sink t. Each node with $p_i > 0$ is connected from s with weight $p_i$, and each node with $p_i < 0$ is connected to t with weight $-p_i$. Every edge in between has capacity infinity. The goal now becomes finding the minimum cut (A, B) in the graph, where A contains s and B contains t.\n",
    "\n",
    "#### The Analysis\n",
    "\n",
    "We now prove that (A, B) is a minimum cut iff A - {s} is an optimal project selection. Note that infinite-capacity edges do not affect capacities of the cuts, since we only count outward edges in c(A, B), and no infinite-capacity edges would cross A-B boundary if nodes in A satisfy all prerequisite requirements. So $$c(A, B) = \\sum_{v \\in B: p_v > 0}p_v + \\sum_{v \\in A: p_v < 0}(-p_v) = \\sum_{v: p_v > 0}p_v - \\sum_{v \\in A}p_v.$$ Minimizing c(A, B) is maximizing $\\sum_{v \\in A}p_v$, the total profit.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Given a weighed graph, network flow algorithms could solve the problem of optimally selecting parts of the graph, either nodes or edges. If you desire an optimal selection of nodes, you should consider minimum cut. If you desire an optimal selection of edges (or flow in it), you should consider maximum flow. After this, you should design a graph so that weights on its edges are relevant to the optimality standard. Remember that edges with minimum capacity and edges with infinite capacity could be used. Adding an additional supersource or supersink is also worth consideration. After the problem is reduced to circulation or network flow, solve it with network flow algorithms (Ford-Fulkerson plus optimal augment path selections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Complexity Theory and Intractability\n",
    "\n",
    "Thus far we only met problems that could be solved in polynomial time. The non-deterministic  polynomial class is a group of problem that we do not know whether it could be solved in polynomial time. Among which are NP-Complete problems, the hardest of NP, which represents practical intractability (polynomial) for its accurate solutions.\n",
    "\n",
    "### 5.1 Polynomial-Time Reductions\n",
    "\n",
    "If any instance of problem Y can be solved using a polynomial number of standard computational steps, plus a polynomial number of calls to a black box that solves problem X, then we say that $Y \\leq_P X$. If X can be solved in polynomial time, then Y can be solved in polynomial time. If Y cannot be solved in polynomial time, then X cannot be solved in polynomial time. \n",
    "\n",
    "Problems in this chapter have even easier reductions: usually they only call the black box once, and add one or two steps of extra computation. Problem reduction is in fact a discovery of fundamental structure of a problem and a proof of isomorphism between problems.\n",
    "\n",
    "#### Example: Independent Set = Vertex Cover\n",
    "\n",
    "An *independent set* of a graph is a set S of nodes that are not connected with each other. A *vertex cover* is a set of nodes S' such that every edge in the graph has at least one endpoint in S'. It is easy to show that independent sets and vertex covers are the same thing: S is an independent set iff V - S is a vertex cover. So Independent Set $\\leq_P$ Vertex Cover, and Vertex Cover $\\leq_P$ Independent Set. The problem of finding the biggest independent set of a graph (and thus the smallest vertex cover) is currently intractable in polynomial time.\n",
    "\n",
    "#### Example: Vertex Cover $\\leq_P$ Set Cover\n",
    "\n",
    "For a set U of n elements and a collection of subsets of U, S($S_1, ..., S_m$), there exists a k cover of U if the union of k subsets in S is U. It is very easy to notice that vertex cover is a special case of set cover. In fact, if you treat E as U, and for each vertex $i \\in V$, $S_i$ contains all edges connected to i, then k cover of U exists iff vertex cover of at most size k exists. \n",
    "\n",
    "Similarly, independent set problem can be generalized into set packing problem: given set U of n elements, a collection $S_1, ..., S_m$ of subsets of U, a number k, does there exist a collection of at least k of these sets such that no two of them intersect?\n",
    "\n",
    "#### Example: The Satisfiability Problem\n",
    "\n",
    "Suppose there is a set X of n boolean variables $x_1, ..., x_n$. A *term* over X is either $x_i$ or $\\overline{x_i}$. A *clause* is a disjunction of terms $t_1 \\vee ... \\vee t_l$. A clause of length l has l terms. \n",
    "\n",
    "A truth assignment for X is a function v: $X \\rightarrow \\{0,1\\}$. An assignment satisfies a clause C when the value of the clause is 1 under the assignment, i.e. at least one of $t_i$'s should be 1. An assignment v satisfies a collection of clauses if it causes $C_1\\wedge ... \\wedge C_k$ (conjunctive normal form, CNF) to be 1, i.e. it satisfies every clause in this collection. In this case, we say that v is a satisfying assignment of clause collection $C_1, ... , C_k$ and the clause collection is *satisfiable*. The existence of such assignment is the concern of the satisfiability problem, which is currently intractable in polynomial time.\n",
    "\n",
    "A special case equivalently difficult is 3-SAT problem, which concerns the satisfiability where each of the collections $C_1, ... , C_k$ are of length 3.\n",
    "\n",
    "We can show that 3-SAT $\\leq_P$ Independent Set. Think about 3-SAT in a different way: What you want to do is just to pick 1 term from each clause so that each clause would be 1. If you can't, it must be because that some terms are in conflict with each other because they are negation of each other. Now we represent 3-SAT as a graph: each term in each clause is a node. All nodes in one clause are connected with each other in a triangle (since only one of them is enough to be chosen), and those terms that are negating each other are in conflict with each other (such as $x_i$ and $\\overline{x_i}$), so we also connect them with each other. Now edges in the graph represent conflicts. What we now need to do is to confirm whether the size of independent set of this graph could be k.\n",
    "\n",
    "Also, if $Z \\leq_P Y, Y \\leq_P X$, then $Z \\leq_P X$, so reductions are transitive, and 3-SAT $\\leq_P$ Set Cover.\n",
    "\n",
    "### 5.2 Efficient Certification, NP, and NP Completeness\n",
    "\n",
    "#### Important Definitions and Theorems\n",
    "\n",
    "- The length of a string s is |s|. A *decision problem* X is a set of strings on which the answers to a problem are \"yes\". An **algorithm** A is a **computable** function that receives a string s and returns \"yes\" or \"no\", so $A: s \\rightarrow \\{0, 1\\}$. (According to Church-Turing Thesis, a function is computable iff its result could be calculated in finite time by a Turing machine.) An algorithm A *solves* problem X if $\\forall s, A(s) = 1 \\Leftrightarrow s \\in X$.\n",
    "\n",
    "- Algorithm A has a *polynomial running time* if there is a polynomial function p such that for all inputs s, A terminates on s in at most $O(p(|s|))$ steps. The set of all problems X for which there exists a polynomial running time algorithm that solves X is the set **P**.\n",
    "\n",
    "- A polynomial time algorithm B that takes two input arguments s and t is an *efficient certifier* of X if there exists a polynomial function p such that $s \\in X \\Leftrightarrow \\exists t, |t| \\leq p(|s|) \\wedge B(s,t) = 1$. The set of all problems for which there exists an efficient certifier is called **NP**. It is clear $P \\subseteq NP$. Whether or not there is a problem in NP such that it is not in P is left to you as a homework problem.\n",
    "\n",
    "- If $X \\in NP$, and $\\forall Y \\in NP, Y \\leq_P X$, then X is said to be an *NP-complete* problem. If X is NP-complete, then X is solvable in polynomial time iff P = NP. So if there is any problem in NP that cannot be solved in polynomial time, then no NP-complete problem can be solved in polynomial time.\n",
    "\n",
    "- If Y is an NP-complete problem, and X is a problem in NP with the property that $Y \\leq_P X$, then X is NP-complete.\n",
    "\n",
    "#### Boolean Satisfiability is NP-Complete\n",
    "\n",
    "Since any computable algorithm could be solved by a Turing machine, which is just the Cartesian product of several finite mathematical objects, any polynomial-time computable algorithm (function) that takes a fixed number n of bits as input and produces a yes/no answer can be represented by a boolean expression that only contains a fixed number (n) of variables and p(n) operators (and, or, not), where p(n) is a polynomial expression of n. Boolean satisfiability is the problem to find certain assignments of variables such that the expression is true. What we now need to show is that $\\forall X \\in NP, X \\leq_P$ Boolean Satisfiability, which is, given any string s, using a blackbox of solving boolean satisfiability, check whether $s \\in X$, which is to check whether $\\exists t, |t| \\leq p(|s|) \\wedge B(s,t) = 1$(We already know what p is). Suppose $ |S| = n$, view B as an algorithm on $n + p(n)$ bits, which is equivalent to a polynomial-size formula K with $n + p(n)$ variables, of which there are p(n) inputs, and the remaining n variables are filled with values of s. Thus $s \\in X$ iff K is satisfiable.\n",
    "\n",
    "#### 3-SAT is NP-Complete\n",
    "\n",
    "Clearly 3-SAT is in NP. We now prove that Boolean Satisfiability $\\leq_P$ 3-SAT. Input values are linked only by and's, or's, and not's. If $x_u$ is labeled with a \"not\", we add a new variable $x_v$ and two clauses $(x_v \\vee x_u), (\\overline{x_v} \\vee \\overline{x_u})$. If $x_u$ and $x_w$ are linked with an \"or\", then we add a new variable $x_v$ and three clauses $(x_v \\vee \\overline{x_u}), (x_v \\vee \\overline{x_w}), (\\overline{x_v} \\vee x_u \\vee x_w)$. Similar logic applies to \"and\". Now we transformed the boolean satisfiability problem into a satisfiability problem of CNFs, each with $\\leq$ 3 terms. Filling dummy terms into clauses with less than 3 terms can transform this into 3-SAT, and we are done.\n",
    "\n",
    "By transitivity of NP-completeness, Independent Set, Set Packing, Vertex Cover, and Set Cover are all NP-complete. \n",
    "\n",
    "### 5.3 Sequencing Problems\n",
    "\n",
    "#### The Problem: Hamiltonian Path/Cycle\n",
    "\n",
    "Given a graph, can you find a cycle/path that will visit each node exactly once? A more general form of this problem (Travelling Salesman) is concerning how to find a shortest Hamiltonian cycle in a weighed directed graph.\n",
    "\n",
    "#### Hamiltonian Path/Cycle is NP-Complete\n",
    "\n",
    "It is easy to check that Hamiltonian Path is NP. We will reduce Hamiltonian Path to 3-SAT. The Graph is on the book. The problem is how this graph is constructed. In 3-SAT, there are n variables, so there are $2^n$ possible assignments. This is naturally fitted into a graph with n pairs of nodes, each pointing to each other, run in a sequence with ends linking to each other, and a Hamiltonian cycle could be constructed. Problem is how to place the clause limit into this graph. We do this via adding \"gadgets\" between each pair of nodes. Create a node $c_i$ for each clause, and it is visited via edges to and from gadgets iff this clause is satisfied. Adding correct edges into the graph construction, and you will get the solution on the book.\n",
    "\n",
    "Reduction of Hamiltonian Path to Hamiltonian Cycle, as well as Reduction of Hamiltonian Path to Travelling Salesman, is easy. Note that Hamiltonian Path problem is searching over permutations of a collection of objects (all cycles), and is concerning whether certain type of permutations exist, which, by some encoding, is but another saying of satisfiability of boolean equations.\n",
    "\n",
    "### Further Reading: Partitioning Problems\n",
    "\n",
    "#### Problem: 3D Matching\n",
    "\n",
    "Given disjoint sets X, Y, Z, each of size n, and given a set $ T \\subseteq X \\times Y \\times Z$ of ordered triples, find the maximum number of triples in T such that each element in these selected triples are covered exactly once. This problem, 3D Matching problem, is a generalization of bipartite matching, and is NP-Complete.\n",
    "\n",
    "#### Problem: 3 Coloring\n",
    "\n",
    "Given a graph, how many colors are required to color each node such that no two nodes with same color are adjacent? This is the 3-Coloring problem, and is proved to be NP-Complete.\n",
    "\n",
    "### 5.4 Numerical Problems\n",
    "\n",
    "#### The Subset Sum Problem is NP-Complete\n",
    "\n",
    "Given natural numbers $w_1, ..., w_n$, is there a subset of $\\{w_1, ..., w_n\\}$ that adds up to precisely W? This problem is solved in 3.3 as a special case of Knapsack by a pseudopolynomial solution. \n",
    "\n",
    "It is easy to check Subset Sum as NP. We now show that this problem is NP-Complete by reducing this problem to 3D Matching. We represent a 3-tuple through fitting 0's and 1's at correct digits of some representation ($w_i$). If there exists a perfect 3D matching, then we can choose some of these $w_i$'s so that their sum is a number that has all its digits 1. If we treat this number as W, then we are done. Conversely, if for some of these $w_i$'s, their sum is equal to W, then there must exist a perfect 3D matching. \n",
    "\n",
    "#### Scheduling with Release Times and Deadlines is NP-Complete\n",
    "\n",
    "Suppose there are n jobs. Each one has a release time r(i) and a strict deadline d(i) and a duration t(i). No interruptions allowed. One job at a time. Find whether or not these jobs are possible to be scheduled. \n",
    "\n",
    "This problem is NP, and we now show that it is NP-Complete by reducing Subset Sum to this problem. If all these jobs have release time 0 and deadline W, and their duration is $w_i$, then Subset Sum is but a special case of this kind of scheduling. \n",
    "\n",
    "### 5.5 Co-NP Problems\n",
    "\n",
    "We know in section 5.2 that for all input s, an efficient certifier B must satisfy $s \\in X \\Leftrightarrow \\exists t, |t| \\leq p(|s|) \\wedge B(s,t) = 1$, which is the definition of NP. Negating this, we get $s \\not\\in X \\Leftrightarrow \\forall t, |t| \\leq p(|s|) \\Rightarrow B(s,t) = 0$, which shows that characterizing $s \\not\\in X$ is a fundamentally different problem than characterizing $s \\in X$.\n",
    "\n",
    "The complement of a NP problem is called a *co-NP problem*. It is easy to prove that if P=NP, then co-NP=NP. People are concerning about the intersection of NP and co-NP. Whether that is P is still open.\n",
    "\n",
    "### Summary\n",
    "\n",
    "We can group NP-Complete problems discussed above into these categories:\n",
    "\n",
    "- Packing Problems. Choose k of some objects with restrictions. Examples are Independent Set and Set Packing (choose some subsets from a collection such that they do not intersect).\n",
    "\n",
    "- Covering Problems: Choose k of some objects to achieve a certain goal. Examples are Vertex Cover and Set Cover (choose some subsets from a collection such that their union is the whole set).\n",
    "\n",
    "- Partitioning Problems: Divide a collection into subsets so that each one occurs in only one of the subsets, with constraints. Examples are 3D Matching and Graph Coloring.\n",
    "\n",
    "- Sequencing Problems: Search over all permutations of a collection of objects, with restrictions of placing them relative to each other. Examples include Hamiltonian Cycle/Path/Travelling Salesman. \n",
    "\n",
    "- Numerical Problems: Examples include subset sum.\n",
    "\n",
    "- Constraint Satisfaction Problems: The most fundamental object of reduction. All previous problems are constraint satisfaction problems. Examples include 3-SAT, a special case of SAT, the most fundamental of all NP-Complete problems.\n",
    "\n",
    "The way for you to prove that a problem Y is NP-Complete is as follows:\n",
    "\n",
    "- Prove that Y is in NP by showing an efficient certifier.\n",
    "\n",
    "- Base on the structure of this problem, find a classical NP-Complete problem X that has some isomorphism with Y.\n",
    "\n",
    "- **Reduce X to Y**, or **Prove $X \\leq_P Y$.** Show that **all** cases of Problem X is equivalent to **some** cases of Problem Y by creating a one-to-one function (injection) from X to Y. Then show that X is solvable **iff** those special cases of Y is solvable (plus a polynomial offset for reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "## 6 Approximation and Online Algorithms\n",
    "\n",
    "For NP-Complete problems or polynomial problems with high degrees, it is nearly impossible to solve them practically. Approximation now becomes a useful technique, which sacrifices accuracy but guarantees fast runtime. The parameter k in k-appoximation is very important: it says that the approximation solution is at most k times the result of the optimal solution.\n",
    "\n",
    "When facing practical problems, usually inputs to algorithms are not given at a single time: the algorithm needs to make the best decision depending on inputs. This is the idea of online algorithms, and they are often the approximation of the corresponding non-online algorithms.\n",
    "\n",
    "### 6.1 Problem: Load Balancing\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "There are m identical machines and n jobs, where job j has processing time $t_j$. The load of machine i is $L(i) = \\sum_{j \\in S(i)}t_j$, where S(i) is the set of jobs assigned to i. Find an assignment of jobs such that the maximum of all loads of all machines (makespan) is minimized. This problem is NP-hard (harder than or equal to NP complete problems), but believe it or not, there are suboptimal solutions to load balancing that could reach a certain multiple of optimal, and one of them is online.\n",
    "\n",
    "#### List Scheduling Algorithm\n",
    "\n",
    "This is an online algorithm. Depending on the job you receive, you could assign the job to the machine with least amount of load currently. The optimal makespan is greater than the length of each job and the average length of each job. Before last job arrives, load of the machine that will receive the bottleneck is less than average. After last job, its workload (makespan) will be less than 2 times optimal makespan, which means this algorithm is a 2-approximation.\n",
    "\n",
    "#### Longest Processing Time Algorithm\n",
    "\n",
    "This is not an online algorithm. Sort jobs by decreasing order, then assign jobs to machine with the least workload. Similar argument would prove that this algorithm would have 4/3-approximation.\n",
    "\n",
    "### 6.2 Problem: Center Selection\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given n points on the plane, set k centers in the plane so that maximum distance r(C) from a set to the nearest center is minimized. This problem is NP-hard, and moreover, finding a $2-\\epsilon$ approximation of the selection is NP-hard.\n",
    "\n",
    "#### The Greedy Approximation Algorithm\n",
    "\n",
    "Pick an arbitrary point as the site for the first center. Then set additional centers at points with the farthest distance from any existing center. By triangular inequality, this algorithm is exactly a 2-approximation of center selection.\n",
    "\n",
    "### 6.3 Problem: Weighted Vertex Cover\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given a graph G=(V, E) with vertex weights $w_i \\geq 0$, find a min-weight subset of vertices $S \\subseteq V$ such that every edge is incident to at least one vertex in S.\n",
    "\n",
    "#### Integer Linear Programming\n",
    "\n",
    "We could formalize this problem using the model of integer linear programming. Make variable $x_i = 0$ if vertex i is not in vertex cover, and 1 if i is in vertex cover. We want to minimize $\\sum_i w_i x_i$, while for every edge guaranteeing $x_i + x_j \\geq 1$. Unfortunately, making the weight of every vertex to be 1 already reduces vertex cover to weighted vertex cover, which means this problem is NP-hard.\n",
    "\n",
    "The goal of integer linear programming is to minimize $\\sum_{j=1}^{n}c_j x_j$ such that $\\sum_{j=1}^{n}a_{ij}x_{j} \\geq b_i, x_j \\geq 0, x_j$ is an integer. While integer linear programming is NP-Complete, general linear programming is solvable in polynomial time using simplex algorithm, ellipsoid algorithm, and interior point algorithms. Using linear programming to approach integer linear programming is a good approximation (we just round the result of LP). If we round all $x_i^{*} \\geq 1/2$ to 1, then the rounding algorithm becomes a 2-approximation algorithm.\n",
    "\n",
    "Someone has proved that if $P \\neq NP$, then there is no approximation algorithm for integer linear programming with its factor less than 1.3606. \n",
    "\n",
    "### 6.4 Problem: Knapsack\n",
    "\n",
    "#### Another Dynamic Programming Algorithm\n",
    "\n",
    "We are now very familiar with the Knapsack Problem. Here we first offer another choice of opt function. If we let opt(i, v) = min weight of a knapsack for which we can obtain a solution of value $\\geq$ v using a subset of items 1,..., i, then our goal is just to check whether opt(n, v) $\\leq$ W. The recurrence relation is now $$ opt(i, v) = min(opt(i-1, v), w_i + opt(i - 1, v - v_i).$$ But this is not in conflict with the NP-Completeness of Knapsack. In fact, this just transforms the enumeration of weight to enumeration of value. \n",
    "\n",
    "#### Polynomial Time Approximation Scheme\n",
    "\n",
    "What if we round up all values by a scaling factor $\\theta$? If we treat $0 < \\epsilon \\leq 1$ as the precision parameter, and $\\theta = \\epsilon v_{max}/2n$ and divide all values by $\\theta$ so that their scale is reduced to reachable, then it is possible to prove that for any $\\epsilon > 0$, the rounding algorithm computes a feasible solution whose value is within a $(1 + \\epsilon)$ factor of the optimum in $O(n^3/\\epsilon)$ time.\n",
    "\n",
    "### 6.5 Online Algorithms\n",
    "\n",
    "#### Problem: Online Matching\n",
    "\n",
    "Even though Bipartite Matching is already studied in Network Flow algorithms (and thoroughly later by Hopcroft Algorithm), sometimes we require an online approximation of the algorithm. Now we present a greedy version: select an edge adjacent to v when that is possible. This is a 2-approximation algorithm. By \"alternating path\" algorithm, any edge in matching $M^*$ is adjacent to at least an edge in M, which is why the algorithm is 2-approximation.\n",
    "\n",
    "#### Problem: Randomized Caching\n",
    "\n",
    "Least Recently Used policy introduced in section 1.2.3 of OS is an online algorithm, which means to evict the least recently used item from the cache, or an unmarked item in the cache. The problem is to evict which unmarked item. An analysis would show that randomly picking one unmarked item from cache would be at most O(log k) times the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Randomized Algorithms\n",
    "\n",
    "There are two types of randomized algorithms: Monte Carlo algorithms and Las Vegas algorithms. Monte Carlo is guaranteed to run in polynomial time, with a probability to find a correct answer. Las Vegas is guaranteed to find a correct answer, with a probability to run in polynomial time. Las Vegas can always be converted to Monte Carlo, but it is unknown whether the other way is possible. Decision problems solvable with one-sided error in polynomial time (Monte Carlo) is called RP. Decision problems solvable in expected polynomial time (Las Vegas) is called ZPP. It is known that $P \\leq RP \\leq ZPP \\leq NP$, but no one knows to what extent randomization helps a problem to be done.\n",
    "\n",
    "### 7.1 Problem: Contention Resolution\n",
    "\n",
    "We start with a very easy example. Suppose n processes want to occupy a common resource, but locks guarantee that only one of them will get the resource at a time. How to avoid starving? Manually order these processes is tricky and error-prone. It turns out that randomized algorithm is good enough to do this: Suppose each process has equal chance to get the resource (1/n). If more than one process tries to get the resource, their attempts would all fail. At a certain time round, the probability that a process i succeeds to access the resource is at least 1/en. After nec ln n rounds, the probability that process i fails to access the resource is at most $n^{-c}$, and the probability that all processes succeeds within 2en ln n rounds is at least 1 - 1/n, which means starving is very unlikely to happen.\n",
    "\n",
    "### 7.2 Problem: Global Min Cut\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given a connected undirected graph, find a cut c(A,B) with the minimum cardinality. This is different from min s-t cut in that s and t is not defined. To get global min cut, network flow solution needs to traverse over all possible (s, t) pairs, that would be $O(mn^4)$ if using Dinitz's.\n",
    "\n",
    "#### Contraction Algorithm\n",
    "\n",
    "Pick an edge (u, v) at random. Contract u and v into a single point. Maintaining parallel edges while deleting self loops. Continue until there are only two nodes. The contraction algorithm returns a minimum cut with probability $\\geq \\frac{2}{n^2}$.\n",
    "\n",
    "Although the above probability is very small, we could run it many times. Running it for $n^2 \\ln n$ times will leave the failure probability less than $n^{-2}$, and you just need $O(mn^2\\ln n)$. Actually best randomized algorithm could reach $O(m \\log^3 n)$ complexity, faster than network flow algorithms. \n",
    "\n",
    "### 7.3 Problem: Primality Testing\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "It is crucial in cryptography to check whether a huge number is a prime. Even though the problem is proved to be in polynomial, we still need a fast algorithm to check the primality of numbers.\n",
    "\n",
    "#### The Miller-Rabin Test\n",
    "\n",
    "Fermat's Little Theorem gave us a way to test: pick a number x less than n. If $x^{n-1} \\not\\equiv 1$ mod n then n must be composite. If $x^{n-1} \\equiv 1$ mod n then n is likely to be prime. From another perspective, this test would give \"prime\" for all primes, and would give \"composite\" for odd numbers whose (n-1) power is not all 1 mod n. But this test cannot distinguish Carmichael numbers (composites whose (n-1) power is all 1 mod n) from prime numbers.\n",
    "\n",
    "Miller-Rabin test adds on the previous Fermat's test by requiring to show a number x that satisfied $x^2 \\equiv 1$ mod n but $x \\not\\equiv \\pm 1$ mod n. If this is the case then n is composite. If n is a Carmichael number, then Miller-Rabin would output \"composite\" with probability at least $\\frac{3}{4}$. The running time of this algorithm takes only $O(\\log^3 n)$ time.\n",
    "\n",
    "### 7.4 Problem: Universal Hashing\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Given a universe U of possible elements, we need to maintain a subset $S \\subseteq U$ such that inserting, deleting, and searching in S is efficient. When U is too large, we would define a hash function $h: U \\rightarrow \\{ 0, 1, ..., n-1\\}$. There will be collisions when h(u) = h(v) but $u \\neq v$, but as long as collisions are not so much, the performance setback is controllable.\n",
    "\n",
    "Problem comes when hash functions are ad-hoc: they are fixed, so hackers may bomb the system with special values, leading to performance dragdown and system crash. An ideal hash function would map m elements uniformly at random to n hash slots, which requires the randomization for the choice of hash functions h (hackers know the randomized algorithm you are using, but does not know the exact choice of the algorithm).\n",
    "\n",
    "#### Universal Hashing Algorithm\n",
    "\n",
    "A universal family of hash functions is a set of hash functions H mapping a universe U to {0, 1, ..., n-1} such that for any pair of elements $u\\neq v, P_{h \\in H}(h(u) = h(v)) \\leq 1/n$, where h is chosen uniformly at random from H. If we choose an S of at most size n from U, and use the above universal hash family, the expected number of items in S that collide with $u \\not\\in S$ is at most 1. \n",
    "\n",
    "How do we find such a universal hash family? Using a prime p as size of hash table, encode all elements x in u using r-digit base-p integers $\\textbf{x}$, then for all vectors $\\textbf{a}$ with components less than p, construct $h_a(x) = \\textbf{a} \\cdot \\textbf{x} \\mod p$. This is a universal hash family. \n",
    "\n",
    "To reach the goal, we need to find a prime between m and 2m, using the algorithm proposed in section 7.3. Then randomly pick number **a** to construct the hashing function. Now, expected number of collisions per operation is less than or equal to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
